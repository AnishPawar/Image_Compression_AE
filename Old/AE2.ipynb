{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define the autoencoder class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # To restrict the output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the hyperparameters\n",
    "input_dim = 784  # Size of MNIST images\n",
    "encoding_dim = 32  # Dimension of the encoded representation\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:07<00:31,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:11<00:27,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.9237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:15<00:23,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:19<00:19,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.9283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:23<00:15,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:27<00:11,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.9278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:31<00:07,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:35<00:03,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.9253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE on training data: 0.9254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the autoencoder\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)  # Flatten the images\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(img)\n",
    "        loss = criterion(reconstructed, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Testing the autoencoder by reconstructing images from the latent space and computing MSE\n",
    "total_mse = 0\n",
    "total_samples = 0\n",
    "for data in train_loader:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)  # Flatten the images\n",
    "    reconstructed = autoencoder(img)\n",
    "    mse = criterion(reconstructed, img)\n",
    "    total_mse += mse.item() * img.size(0)\n",
    "    total_samples += img.size(0)\n",
    "\n",
    "average_mse = total_mse / total_samples\n",
    "print('Average MSE on training data: {:.4f}'.format(average_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE on training data: 0.9254\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)  # Flatten the images\n",
    "    reconstructed = autoencoder(img)\n",
    "    mse = criterion(reconstructed, img)\n",
    "    total_mse += mse.item() * img.size(0)\n",
    "    total_samples += img.size(0)\n",
    "\n",
    "average_mse = total_mse / total_samples\n",
    "print('Average MSE on training data: {:.4f}'.format(average_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 784])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reshaped_tensor = reconstructed.view(96, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape\n",
    "img_new = img.view(96, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x171ac7040>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_new.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1779eed30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reshaped_tensor.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define the autoencoder class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28x28x1 -> 14x14x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # 14x14x16 -> 7x7x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 7x7x32 -> 4x4x64\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 4x4x64 -> 7x7x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 7x7x32 -> 14x14x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14x14x16 -> 28x28x1\n",
    "            nn.Sigmoid()  # To restrict the output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:26<21:46, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:55<22:29, 28.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:25<22:34, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:53<22:00, 28.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [02:22<21:23, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:52<21:15, 28.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [03:19<20:30, 28.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [03:53<21:05, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [04:22<20:24, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [04:52<19:53, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [05:22<19:30, 30.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.8861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [05:34<19:46, 30.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m img, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed, img)\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the autoencoder\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(img)\n",
    "        loss = criterion(reconstructed, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the autoencoder by reconstructing images from the latent space and computing MSE\n",
    "total_mse = 0\n",
    "total_samples = 0\n",
    "for data in train_loader:\n",
    "    img, _ = data\n",
    "    reconstructed = autoencoder(img)\n",
    "    mse = criterion(reconstructed, img)\n",
    "    total_mse += mse.item() * img.size(0)\n",
    "    total_samples += img.size(0)\n",
    "\n",
    "average_mse = total_mse / total_samples\n",
    "print('Average MSE on training data: {:.4f}'.format(average_mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transform),\n",
    "    batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the autoencoder\n",
    "model = Autoencoder()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5529, 0.5603, 0.5532,  ..., 0.5603, 0.5525, 0.5546],\n",
       "          [0.5466, 0.5467, 0.5442,  ..., 0.5476, 0.5454, 0.5498],\n",
       "          [0.5524, 0.5579, 0.5517,  ..., 0.5591, 0.5525, 0.5552],\n",
       "          ...,\n",
       "          [0.5467, 0.5472, 0.5460,  ..., 0.5481, 0.5463, 0.5502],\n",
       "          [0.5523, 0.5593, 0.5519,  ..., 0.5596, 0.5523, 0.5547],\n",
       "          [0.5542, 0.5592, 0.5536,  ..., 0.5590, 0.5534, 0.5600]]],\n",
       "\n",
       "\n",
       "        [[[0.5529, 0.5604, 0.5532,  ..., 0.5603, 0.5525, 0.5546],\n",
       "          [0.5466, 0.5467, 0.5442,  ..., 0.5475, 0.5454, 0.5498],\n",
       "          [0.5524, 0.5580, 0.5517,  ..., 0.5591, 0.5525, 0.5553],\n",
       "          ...,\n",
       "          [0.5467, 0.5472, 0.5459,  ..., 0.5481, 0.5463, 0.5501],\n",
       "          [0.5523, 0.5593, 0.5519,  ..., 0.5595, 0.5522, 0.5547],\n",
       "          [0.5541, 0.5593, 0.5536,  ..., 0.5590, 0.5534, 0.5599]]],\n",
       "\n",
       "\n",
       "        [[[0.5529, 0.5604, 0.5532,  ..., 0.5603, 0.5525, 0.5546],\n",
       "          [0.5466, 0.5467, 0.5443,  ..., 0.5477, 0.5456, 0.5501],\n",
       "          [0.5524, 0.5579, 0.5517,  ..., 0.5590, 0.5525, 0.5553],\n",
       "          ...,\n",
       "          [0.5467, 0.5472, 0.5460,  ..., 0.5481, 0.5463, 0.5501],\n",
       "          [0.5523, 0.5593, 0.5519,  ..., 0.5595, 0.5522, 0.5547],\n",
       "          [0.5542, 0.5592, 0.5536,  ..., 0.5590, 0.5534, 0.5600]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.5529, 0.5603, 0.5532,  ..., 0.5603, 0.5525, 0.5545],\n",
       "          [0.5466, 0.5467, 0.5443,  ..., 0.5476, 0.5456, 0.5500],\n",
       "          [0.5524, 0.5580, 0.5518,  ..., 0.5592, 0.5525, 0.5552],\n",
       "          ...,\n",
       "          [0.5466, 0.5475, 0.5460,  ..., 0.5480, 0.5463, 0.5502],\n",
       "          [0.5523, 0.5593, 0.5521,  ..., 0.5595, 0.5524, 0.5547],\n",
       "          [0.5542, 0.5592, 0.5536,  ..., 0.5587, 0.5533, 0.5600]]],\n",
       "\n",
       "\n",
       "        [[[0.5529, 0.5604, 0.5532,  ..., 0.5603, 0.5525, 0.5545],\n",
       "          [0.5466, 0.5467, 0.5442,  ..., 0.5475, 0.5455, 0.5499],\n",
       "          [0.5524, 0.5579, 0.5517,  ..., 0.5592, 0.5525, 0.5552],\n",
       "          ...,\n",
       "          [0.5467, 0.5472, 0.5459,  ..., 0.5481, 0.5463, 0.5501],\n",
       "          [0.5523, 0.5592, 0.5519,  ..., 0.5595, 0.5523, 0.5547],\n",
       "          [0.5542, 0.5592, 0.5537,  ..., 0.5590, 0.5534, 0.5599]]],\n",
       "\n",
       "\n",
       "        [[[0.5529, 0.5603, 0.5532,  ..., 0.5603, 0.5525, 0.5546],\n",
       "          [0.5466, 0.5467, 0.5442,  ..., 0.5476, 0.5456, 0.5500],\n",
       "          [0.5524, 0.5580, 0.5517,  ..., 0.5590, 0.5526, 0.5553],\n",
       "          ...,\n",
       "          [0.5466, 0.5473, 0.5461,  ..., 0.5481, 0.5463, 0.5502],\n",
       "          [0.5523, 0.5593, 0.5519,  ..., 0.5595, 0.5523, 0.5546],\n",
       "          [0.5542, 0.5592, 0.5536,  ..., 0.5590, 0.5534, 0.5600]]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of target should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of target should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the autoencoder\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the autoencoder\n",
    "test_images, _ = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(test_images)\n",
    "\n",
    "# Visualize the results\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i].squeeze(), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed[i].squeeze(), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the convolutional autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Assuming grayscale images\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output pixel values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset and preprocess if necessary\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor\n",
    "    transforms.Resize((32,32)),\n",
    "])\n",
    "\n",
    "# Example of loading MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Autoencoder()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.10183986931244532\n",
      "Epoch [2/30], Loss: 0.1007771303097407\n",
      "Epoch [3/30], Loss: 0.10077713034947713\n",
      "Epoch [4/30], Loss: 0.1007771303097407\n",
      "Epoch [5/30], Loss: 0.10077713067134221\n",
      "Epoch [6/30], Loss: 0.10077713063160579\n",
      "Epoch [7/30], Loss: 0.10077713046471277\n",
      "Epoch [8/30], Loss: 0.10077713048458099\n",
      "Epoch [9/30], Loss: 0.10077713056405385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)  # Compute the loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        # img = img.view(img.size(0), -1)\n",
    "        # img = img.to(device)\n",
    "        output = model(img)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABiEAAAE/CAYAAAAg+mBzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6AElEQVR4nO3deZTdZX0/8GcC2RPIwpo9ECCEhEWWssgqQkSgIBUFRMEibY8KWnr0nCr9cTyeqlXbSlHqUdSqLFZlURaRsMi+hyVAIBAIgSxAIAtZSELm9wcn0c/n3s5kYL73zmRer3/wPXPnfp84T57v8uR+Pi2tra2tBQAAAAAAoJP1avYAAAAAAACATZNNCAAAAAAAoBI2IQAAAAAAgErYhAAAAAAAACphEwIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBK2IQAAAAAAAAqsfnGvGjdunVl3rx5ZfDgwaWlpaXqMdGFtba2lmXLlpURI0aUXr2q3cMy71ivUfPOnOMvmXc0mnMszWCto9GsdTSDtY5mMO9oNOdYmmFj591GbULMmzevjB49utMGR/c3d+7cMmrUqEqPYd6RVT3vzDnqMe9oNOdYmsFaR6NZ62gGax3NYN7RaM6xNEN7826jtsUGDx7caQNi09CIOWHekVU9J8w56jHvaDTnWJrBWkejWetoBmsdzWDe0WjOsTRDe3NiozYhfKyGrBFzwrwjq3pOmHPUY97RaM6xNIO1jkaz1tEM1jqawbyj0ZxjaYb25oTG1AAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQic2bPQAAAAAAuq9eveK/cZ04cWLIhx56aMiLFi0K+b777gt5zpw5nTg6AJrNJyEAAAAAAIBK2IQAAAAAAAAqYRMCAAAAAACohJ4Q9Hj9+/cPeccddwx5woQJbX6/T58+bb7/mjVrar62dOnSkF988cWQZ8yYEfL8+fNDfvvtt9s8Jj1b3759a742fvz4kPfZZ5+Qhw8fHnJLS0vIra2tbR7z0UcfDXn69Ok1r1myZEmb7wHQCL179w75+OOPDzmf12+//faQX3755WoGBgDdyGabbRbyiBEjQj777LNDnjp1asizZs0KOfeUePPNN2uOmftIQEeNHDky5P333z/kes9a7r777pBfeeWVzh8Y9AA+CQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACV0JiaHmfYsGEh/9Vf/VXIRx11VMjjxo0LecyYMSHnhly5kVG9xtS5yVZuPD1z5syQ77zzzpBzE+Dc6HrdunU1x2TTkRtP56bTEydOrPmZgw46KOSTTz455NGjR4ecG1NnuVH1ddddF/J///d/1/zMrbfeGvKKFSvaPAbssssuIW+11VYhz5kzJ+SXXnqp8jHR/fXr1y/kL33pSyEPGjQo5M997nMha0xNZ8vNWPM5OK99Q4cOrXmPfD3aUc8//3zIztGbvv79+4ec75G23nrrNnMppQwcODDkfB+0bNmykBcsWBDykiVLQs73SCtXrgx57dq1NWOgeQYPHhzyBz7wgZBPP/30kIcMGRJynj/5nvaNN96oOeaNN97Y0WFCMGXKlJDPOeeckOvNu9mzZ4esMTW8Oz4JAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCX0hKDHybXvP/7xj4ec6+vffPPNIf/Hf/xHyIsWLQo594Do06dPzRhGjBgR8oEHHhjyqaeeGvLBBx8c8s9+9rOQ77jjjpBzvdVcn5XuJdfs3W233UI+88wzQz722GNr3mObbbYJOfeVaE/uAZFNnTo15Hq1NHOvk+eee65DY6Brqzen8tdWrVoV8urVq9t8z+OPPz7kPLd/+ctfhvzzn/+85j3eeuutNo8B+Ty93XbbhZxrWG++ebx8VqOcLM+R3MOhd+/eIedzdO5bss8++4Q8efLkmmN29Lye5V5Ojz32WMi5Nj9dT+4tkufR8OHDQ859l/I8O+CAA0J+3/veV3PMUaNGhZzP67nXSO5z98QTT4T87LPPhvzII4+EPHfu3Jox0Di598zIkSND/sQnPhFy7rGUe4DkOXnccceFXK/X12233Ray6zw6Kj8Pyr1JZs2aVfMzr732WqVjgs6QrwNyzr2fci/Z3MeplM5fY30SAgAAAAAAqIRNCAAAAAAAoBI2IQAAAAAAgEroCUGPM3v27JC/8Y1vhLx8+fKQcy3K9mrjb4wZM2aEfOutt4Z8xRVXhPy1r30t5Isuuijkb3/72yFfeumlIb/88ss1Y+iMPwedo6WlJeRcK3rvvfcO+Utf+lLIhx12WMi5/moptfX+cs3e9sbQnvzzuR52KbV1ZOnech393KuklFJ22GGHkKdPnx5ye31Bcm3oLbfcMuRcF3377beveY8XXnihzWPQ8+QeDrfcckvIZ599dsgTJ04M+Z577gk592Fi07Ix57e8Hua16JRTTgk59x05+uijQ871quv1F+ts+Trhu9/9bshPPvlk5WPg/5bnYSm112q5vv6kSZNCPuOMM0I+8sgjQ95qq63aPGaeI6W0Xyt6woQJIe+8885tHuPFF18M+YILLgg53+OU0n5/KTpP7m+z7777hnzEEUeEnHslXn311SHna8eddtop5LxWllLK2LFjQ37mmWf+7wFDqT2H7rnnniHntXPevHk171Hva1C19p7R5Lmd+9jl3lCnnXZayCtWrAj5+uuvrxlD7t30XvkkBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJXQE4IeZ9myZSHnOpK5V0IjeifkepmzZs0KOdenzj0B8vcHDBgQcu4hUUopr776aofHSefo1Svu/+Y698cff3zI//RP/xRyrpe6Mf0b5s+fH/K9994bcq4XmOtT16uB/Zdef/31kHNN31JKWbRoUbvjpPvIdYA/97nP1bxm3LhxIf/rv/5ryO31hIAq5PP6qlWrQs5rdF7/8vfZtG299dYhH3LIITWvOfTQQ0POa1+uvd9ejd9mzLG8puf+ADTXFltsUfO1XH//H//xH0PeY489Qu7bt2+bx8j3SAsXLgy5Xu39OXPmhDx48OCQ89+XXM8/9wsbNWpUyLmHxLBhw2rGoC9PNerNuQ9+8IMhn3/++SHne9obbrgh5G9961sh77777iEfd9xxIde7n8h9naA9I0aMCDn3InG+o6vKfXGmTp0a8oknnhhy7mM3cODAkPO6vnTp0pDr9fDUEwIAAAAAAOgWbEIAAAAAAACVsAkBAAAAAABUYpPuCXHggQeGnOuxllJbh3DChAmdOoZcx/DSSy+tec2vfvWrTj0mHbNu3bpmD6HG22+/HfLixYtD/t73vhfy+PHjQ87zONc9LKWU22677d0PkPdkyJAhIX/sYx8L+Ytf/GLIY8aMCblPnz4h5xq9d911V80x8+/7+eefDzn3GemoK6+8MuRf/OIXNa/J85juLf8+ly9fXvOa/v37h1yvzmRbJk2aFHL+uwOdIV8HrFixIuTc82blypWVj4muI9e0//u///ua1+y1114h5/N07rvUnlyjd2N6RDz99NMhP/bYYyGfeuqpIef+ALn2fr168DTOLrvsEvJpp51W85r8O811z/M8fPLJJ0PO96X5+jH3j3vrrbdqxrB69eqQc1+622+/PeQLL7ww5EGDBoWc53p7mc6Ta4fnWuOllHLeeeeFnHvmTJ8+PeRvfvObIeceIu31rMs9JkqpPSdDe3Ivk9x7Jte8f/zxxysfE91f7u+V+3zmZ835mV1+xjN58uSaY7R3L5yPme+98xjz+vnwww+HPHPmzJoxdDZncQAAAAAAoBI2IQAAAAAAgErYhAAAAAAAACrRrXtC7LjjjiHnWm8f/vCHQ54yZUrNewwbNizkwYMHd9Lo3pHHmOtellJb2/LWW28N+Zxzzgk51wi9+OKLQ77vvvtCXrZs2cYNlm5j4cKFIee6v0cffXTIe++9d8176AnROJtvHpfacePGhfz5z38+5B122CHkvEbkWvxXX311yJdffnnNGPKcGTt2bMi5FnSu3Z/rCS5YsCDkXDsz95wopbbXCd3bNttsE/K2227b6cfYaqutQu5oXXV4N958882Q83rnuqpnyefwXG+3lNpa6kuWLAk5n6dzTd7W1taQc+3+fMx6/cxmzJgRcl4vTzrppJBzTwjn6OaaOHFiyGeddVbIH/3oR2t+ZujQoSHna7Fp06a1mXNPsdwDIvd7qCf3BNh3331Dzn+OPC9zD57cp+Kaa64JWT+A6uR+Hvl+pJTa5xCvvfZayLnXZV6Xcl+RVatWhZz74UBn2GmnnULO99YPPPBAyHpCUE/us5Sfn3z5y18OOa+huddWfjacz+ml1PZ8yH1ycg/im2++OeSnnnoq5JdeeinkuXPnhjx79uyaMXQ2n4QAAAAAAAAqYRMCAAAAAACohE0IAAAAAACgEl26J0Su1TZmzJiQzz333JAPO+ywkMePHx/yc889V3OMK664IuRc97ejjjnmmJD322+/kHOdzHp23XXXkD/1qU+FvP3224d8ww03hPzII4+ErHbxpu+FF14IOdcKHj58eANHQ5ZrPee6yznn319eC/NalvvA5PqrpdTW799///1DHj16dMi5B0Qe0z333BNyrp25cuXKmjGwaWmvrmUpteefvFa1J9diz38XoAq53mquWb527dpGDocme/DBB0P+yU9+UvOa+++/P+Tcu+m6664LOfcdyXIfp969e4ecrytKqa2Vn+8n8nk8v0euiZ37A9C58vns0EMPDTn3Oqx3/rvyyitDvuqqq0LO12bt3S+0J/c0K6WUY489NuRTTz015Hy9mXsAfP/73w8535vnetb55+k8uZdl7lNSSm3PuLzW5fuDfD9Qb+2Czpbn6e677x5y7j3y8MMPh5zr7NMz5d5Z+brqi1/8YsgnnHBCyPXujduS+zWUUtvjIfeCzb2dnnjiiZDnz58fcr43z72fOnpd8G64mwcAAAAAACphEwIAAAAAAKiETQgAAAAAAKASXbonRK7llvsr/PVf/3XIuWfE7bffHvJPf/rTmmP84Q9/CPm99oR4+umnQz777LNDPvzww2t+JveJmDBhQsi5B0Su65Vrh6nF3vPkOrG5nj/NlXs+5L+z//Vf/xXy1KlTQ95uu+1CnjZtWsh53an3+889co488siQ+/fv3+Z75HqBuebrrFmzQlbzddM3bNiwkLfccsua17z88ssht1djNa9lQ4YMCTnPy+XLl4esVjTQ2Z599tmQ6/VKuO2220LOfUUaUV968ODBIef7idxjJ6+nN910U8jz5s3rxNGR5fNdrse/9dZbh5yvs0qp7Qnx+9//PuSOXovl3iN5TMcff3zNz5xyyikh55rZuY9Zvob94Q9/GHLuW5Gvoek8/fr1C/mggw4KeZ999qn5mdwDItfSnz17dsiNqC8O+blhfr6W+5vkdSbfr+ReYPRMo0aNCvm0004L+cQTTww5z6snn3wy5DyvBg4cGHJeP0up7f+be38uWrSo5me6Op+EAAAAAAAAKmETAgAAAAAAqIRNCAAAAAAAoBI2IQAAAAAAgEp06cbUuTnW0UcfHfIWW2wR8lNPPRXyRRddFPKvf/3rThxdfblxyA477BDy5MmTa34mN47LDWJzI+pLL7005EceeSTk3KiTTd/IkSNDzk1uNGvtWnIDoUsuuSTkxx57LORx48aF/Pjjj4f8/PPPh5ybKJVSyiGHHBLy3nvvvVFjXS83lttmm21C3n///UOuN+dyc+s33ngj5IULF4b8yiuvhJwbfdJYW221Vci77bZbyAMGDKj5mdxIs73zU36P0aNHh5znYZ4jS5cubfP9YWPk6888L3OD37Vr11Y+JrqOJUuWbNTXqpSbcJZS20T4Ix/5SMj5/iLLDWa7Y7PD7iSfz15//fWQcwPLPn361LxHvt7v27dvyO1d/w8dOjTkKVOmhHzssce2mUupnXe5EfVll10W8k9/+tOQFyxYELJG1I2T7x/zvcH48eNrfmb69Okh//a3vw05X5dBI+S179RTTw15yy23DDmvU+Yt9YwZMybkE044IeSWlpaQr7jiipDz+rh48eKQhwwZEvLKlStrxjB37tyQ87VDd+STEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFSiS/eEyLUvTzrppJBzbbevf/3rIf/pT3+qZmBt6NevX8iDBg0KOderq2fZsmUh/+EPfwj5ggsuCDnXDGXTk/8u5PrUY8eODTnPidxXhK4l17+9995728xZXguPPPLImtecfvrpIW+33XYdGWLN2vbZz3425M985jMh16tXmGtm594Wd9xxR8jXX399yM8++2zIb731VhsjprPts88+Ie+1114hv/zyyzU/c+ONN4acf2e9esV/C5HrE2+//fYh5xrl+ZjOh2yMXMM193jItfNzzdb8/XzdBp0tr5XbbrttzWsOP/zwkE888cQ23yOfk/O1Yr3axHSefJ103333hXzooYeGnH+/pZTyt3/7tyHPmTMn5BdeeCHkvNYdeOCBIZ955pkh535f+edLKeXJJ58M+corrwz5u9/9bsjmVdex++67h5z7cNXrz5Hn2K233tr5A4N25PNZvhfO6+ebb74Z8i233BJyvseEUmrnWc75vjb31sr9HHJvxNwTqafwSQgAAAAAAKASNiEAAAAAAIBK2IQAAAAAAAAq0aV7QnTU7NmzQ3711Vc7/RibbbZZyLnHQ66tmGttjho1quY9cy2x559/PuQf/ehHbb6e7i/Pq4EDB4Y8YcKEkPfYY4+QDznkkJBzXcNcv7OUUoYPHx7yqlWrQl69enXIa9asqXkPuob8+//4xz9e85pdd921Q+/Z2toa8tq1a9t8fZ7DOZdS24di3LhxIR911FEhT548OeSvfvWrIec6i1Rrl112CTn3a3j00Udrfua5555r8z3zPMlrW669f88994S8cOHCkPPaWUrt3HUOJfeEGDZsWMh5jrz22msh6z3Ss7TXQ6SU2rUs1/vPOZ9jc+31XHc4z9GPfOQjNWPIvZpyX7o8r6+77rqQ33jjjTbHSLVybf18D7n33nvX/Ezu1fStb30r5OnTp4e89dZbh5z7TAwdOjTkPC/vv//+mjFcfPHFIf/qV7+qeQ1d00477RRyvq57/fXXa34m9xlpr8dHezXV8/qa1VuH2ltf2fTl+4M999wz5EmTJoV8zTXXhDxjxoyQc88IKKX2uilfJ+Vnu5/73OdCzvfB+RldT127fBICAAAAAACohE0IAAAAAACgEjYhAAAAAACASmxSPSEmTpwYcq5rOG/evA6/Z67xuu+++4b8mc98JuQjjjgi5JEjR4Y8f/78mmNcdtllIX/ta18LOdcezvU56X5yHcO99tor5P/3//5fyLlWfq4ft2DBgpBzjdf88/V+JtdGzL1IbrzxxpD1iOg6cr+HXAezlI7XXF2+fHnITz75ZMg333xzyBuzvuaeOXn9zOvtDjvsEPKAAQNCzn8m9as7V/7/O/dr6NevX8jPPPNMzXu89NJLnTqm3JfiC1/4Qsi5l00ppTz00EMhf/vb3+7UMdH95Jr+hx12WMh57ud55TqsZ8n9jPJ8KaX2vJvPmfn6P9cVfvrpp0POdYY/+clPhnzyySfXjGH8+PEh5+u0XMv9+9//fsivvPJKzXvSPD//+c9DHjx4cM1rzjnnnJD333//kHPPiPb6d+Xz5eWXXx5y7iNSSm0vQ7qPESNGhLzllluGnNepUmrvH/P5Mfd8GD16dMj5eU2+J87q9ZzIvQ5nzpzZ5nuw6dliiy1Czs868nVe7iOY73Ohnvx846abbgo5X3cdeeSRIZ955pkh//CHPww5X5f1FD4JAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCU2qZ4QuS5mrr96/fXX1/xMrlE+ZcqUNr9/wAEHhLztttuGnOt15jpiP/vZz2rG8IMf/CDkZcuW1byGTUvv3r1Dzv1LDj300JDznPjGN74R8q233hry2rVrQx44cGDNGIYNGxZyriP7z//8zyHnesT/8i//EvKsWbNCVjO7cV599dWQ69V1zjWtV61aFXLuCfL1r3895FzvP9eJ3ZgeIYsXLw4594TIdWT33HPPkIcPHx5yrvepT0nn2mqrrULOPY7yPHvuuedq3iOfE/PvNPeZOOOMM0LeeuutQ95mm23aHNMdd9xRM4bHHnus5mvwl/I5ub0eOmxaxo0bF3Ku6ZtrTed1rJRShgwZEnI+x+a66UuWLAl59uzZIedz9o477hhyXp9LqT0HPvrooyGff/75bX4/j5nmyvexuQ5+KbXzKtfXz9dJ2Z133hny9773vZBvv/32kOv1CHDt1X3kvlo777xzyLknRF6nSqmtgf75z38+5P3226/NY+T7zzxHc3+3eveT+bruoosuCjnfF9P95euy/AwuP6fI/W1+9atfhZx7RND95DmR16/8/OzdPJvKvQ1zz9T8rPeb3/xmyMccc0zIDz/8cMj5nFpvzd0U+SQEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlejSPSFWrlwZcq5Rft5554Wc66fm73/84x+vOUauSzh06NAOfb+9usG5fmeuv1pKKQsWLGjzPdj0rFixIuR777035FNOOSXkXKc318LMtdlzzbvNNtusZgy5buxTTz3V5phyrcW/+7u/CznXyHvyySdrjkk1br755pAHDRpU85rcz2bmzJkh33fffSHffffdIb/55psh55qtG6O9fjd5Pc29THLddqqVf1+vv/56yJMmTQo592UqpZRTTz015FyvM59Tx44dG3K/fv1CzrU0f//734c8bdq0mjFYi6Bn6dOnT8i558MRRxwR8iGHHBLy5MmTQx49enTIeV0qpXZ9XLRoUcgTJkwIecyYMW1+v2/fviHnP1O9/g35PH7hhReGfNddd4W8fPnymveg6xg1alTIO+ywQ81r8jzpqDxnHnnkkZBzz7F3c+1H15HvD/J1du7PkOvul1LKhz70oZDzepqfnbz22msh52uypUuXhpz7VuRcSimHH354yHl9XLduXch57cu9E+n6RowYEXKeh7kv0yWXXBJy7m341ltvdd7gaIi8PuW+gBdccEHI+Z7xmmuuCfnFF19s95i571L+mXwfmp83T5w4MeR8rZeft+kJAQAAAAAA8B7YhAAAAAAAACphEwIAAAAAAKhEl+4JkWtwXXbZZSHnmvZTp04NeZtttgl57733rjlGrhn4pz/9KeT7778/5EMPPbTNPHjw4JBfeOGFNjM9U65F+fLLL4d81VVXderx8jwvpZQ1a9aEnGty5pp3uabyscceG3L++6YOe+PMnTs35KuvvrrmNXkty71ocn6vtTJ33HHHmq/tv//+bf5Mnqe5FnHuE6Q2cbVyT4j//d//DfmNN94IuV696lxvOJ/X8zkx11rPvZ5y/5M8plzztRS1f+m4fD7M50uap14vtlw7P98PHHjggSG/733vCznXmp43b17Iv/vd70JevHhxzRhyHfT8nvl8lXscdbTnUb0eSzNmzAg539PoAdG15H5t+T71xBNPDDnP61Jqz28vvfRSyHleDhgwIOTtt98+5C222CLkXr3iv1fMPefoXvIakHsU5vmUr+FKqX3+kns+/OY3vwk51zzP13352j73yzn66KNrxtDeGp97Kz7wwAMhuy7s2nL/uFJKef/73x/yySefHHK+/r/ppptCztd1dD+590vur5CfTe25554h52cbuZ/D/Pnz2x1DXjvy85Nrr7025Pw8ZOuttw45n5N7Cp+EAAAAAAAAKmETAgAAAAAAqIRNCAAAAAAAoBJduidErp+aa7T+7Gc/C3nmzJkh5xr1Q4YMqTlGez0hpk+fHnKuU5hrEN5zzz0hT5s2LeQ5c+bUjAG6glz3ddCgQSHnuZ/r8tE4ucZvrtGb18pSavtGdLbcD2e//farec3hhx/e5nvk9fj5558POdfzVJu4WrnuZT6fzZo1K+Rcl72U2nrC+Xecf4ff/va3Q849JB599NGQ8xxR55fOMHv27JDr1d+nOcaOHVvztVw7/4wzzgg596t5/fXXQ77llltCfu6550J+5ZVXQq53P5H7ZuVx5n43WT5v52uwnOtdg2211VYhjx49OuTcgyDXctdnqbHynMl17E866aSQ682hPHcfeuihkHPvwkMOOSTkgw46KOR8H5yvHXOvLrqXhQsXhpyfneQa6rkvV733uOaaa0K+8MILQ87n03xdlz3yyCNtfr+UUvbYY4+Qp0yZEnL+c+S1j65t5MiRNV/LPSHGjx8f8g9+8IOQ8znV/UH3l3uCbb5524+yJ02aFPLpp58ecl4X7rjjjpr3yP0Pc1+Jvn37tvmeuadcPof21F5dPgkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlejSjanbk5tldUbj1dxMJDd123fffUMeMGBAyFdccUXI1157bciLFi16r0OETpHnbm7cePDBB4d8wgknhJyb1GpW13lyk6Pddtst5LwOvfjiiyE/8MADNe+ZG1B2VG4GlZtjH3DAASEfc8wxNe+x0047tXmM3LT42WefDTk3b9JEs7Fyc94ZM2a0mTfGlltuGXJu+JobV7/55psht9fgEEoppVev+G9ucsP03r17h5yvJ3tq47iuYMyYMSHnJtSllPLpT3865MmTJ4ec15HcaDo3Qs3XR0cccUTIuQlqKaWMGjWq5mttHTM3a73nnntCzk2mcyPWCRMm1Bwjn4fPOuuskJ9++umQ87XC448/HvKKFStqjsG7l9edPJePPfbYkPM8zNfdpZRyySWXhPzggw+GnJus5+vJPG+POuqokJ944omQ611LuhbrPnKT1enTp4ec7/3qNaZevHhxyLkZem523VG5kWu9hsL5niTfP7hf6F4GDRoUcj7flVLK3nvvHXI+h1522WUh5wbCdH95bViwYEHIr7/+esj5HjM3N8/n5HrXdvl+IF8X5bXowx/+cMj5GU1eL/O1YU/hkxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUolv3hKhCrh123HHHhZxr1OUag7k+nR4QNEPubZLrrJdSyvve976QTz755JAPO+ywkB977LGQv/Od74Q8a9asDo6S9XK98vHjx4d8/vnnh3z44YeH/Kc//SnkzTevXdrvu+++kNurrZ9rEW+zzTYhH3TQQSGfeeaZbX6/3rhyDdfccyDXTVSfuvvLcz3Xgc11fPM8zbU385zKtd+hlNqeDyNHjgw5nyPzOZTmmTp1asif+cxnal6z6667tvke+fe5yy67hHzeeeeFPHjw4JDzOlOvR8i8efNCnj9/fsh33XVXyJdffnnIuY56HnPus3TKKafUjGGvvfYK+Ywzzgg5n3Pvv//+kP/hH/4h5NyXiY7J57vtt98+5NNPPz3k3IfwD3/4Q8g//vGPa45xyy23hJx/x1dffXXIH/vYx0LO15O5J8Cdd94Zcu4bUkopS5curfka3UN+bpHrn+f+OqXUztPcM/CPf/xjyLlnYL0eD39p+PDhIed7onqvyWtynqeuDbu2HXfcMeQPfvCDNa/J/Wt++ctfhvxu+tLRveS/x7kHRH7umteqfC+Qr5n23HPPmmPmZ715DEuWLAk5Pw/7yU9+EnJ+ntZTexv6JAQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACV6PE9IXK9zgkTJoR84YUXtvnzuc56rk1G15fnQK7Du2bNmkYOp648xlyfOOetttoq5I9+9KM17/nZz3425C222CLkf/u3fwv5e9/7XshvvfVWGyOmI3KNwmOPPTbk/fbbL+T8u8qvHzt2bM0xfve734V8xx13hDxnzpyQc53ET3/6022OKffTqae9HhAPPPBAyLnXRe4PQPczcODAkPfff/+Qt91225BzbeEXX3wx5J5aS5OOyefIPM9yb5Lce4TmmThxYpv53cg9H3KN31yz/LXXXgv57rvvrnnP3OPh5ptvDjnXze/oteWvf/3rkHM/h1Jq62jnvhG53nE+j7f39yDXRqZj+vXrF3Lfvn1DzveQV111VcjTpk3r8DHzOTLXUZ8yZUrI+frx/e9/f8gPPvhgzTFy3wi6j/wcI19z5ecipdT24DnwwAND/vrXvx7yj370o5Bzj4i8/p544okhf/KTn6wZw4gRI0LOddZ/+tOfhuxasWvJ12S77757yDvttFPNzzz88MMh5+cSbPryc4R8zly4cGHI+VlVft6S1bsuy++R16vcj+ass84KOfeI6ArPFbsCn4QAAAAAAAAqYRMCAAAAAACohE0IAAAAAACgEj2+J8QOO+wQ8mmnndahn3/88cdD1hOi6xswYEDI48ePDznXR/3tb38bchW13HLd3Vw3dsyYMSEfdthhIR9wwAEh5xquw4cPrznmrbfeGvK3vvWtkB955JGQ9YCoTv7957r5uU9Jez+/22671bwm19c899xzQ851FnO9zj59+rT5/Y2Re0Dk2sJXXHFFyDNnzgzZHOz+cn+bPNfz9/PczrX8c3+UJUuWvNchAl3Iyy+/3GYupZRRo0a9p2M89NBDIed+C9ddd13IuX9RKbU9H3JfiffaTyG/3/PPP1/zmv/5n/8J+corrww5X/9m8+fPD1kPiMbKvUfynHo38rXdk08+GfKiRYtCHjlyZMijR49uM9O95Xva3Ick94kppZRzzjkn5Nyf4WMf+1jIJ5xwQsjtrSu5bnuew6XU9oC4+OKLQ54xY0bIuY47zZX7F51xxhkh5+dzpdT2YnrllVc6fVx0LfnZQ76GWbFiRci5/2Ve39rrX5mvs0opZe7cuSHfdtttIee16Jlnnmn3PfFJCAAAAAAAoCI2IQAAAAAAgErYhAAAAAAAACrR43tC5Lrmffv2DTnXEMw9IC655JKQ69VopWsZPHhwyIccckjIF1xwQch77LFHyA8//HDIuYbu6tWrQx42bFjNGHIfilxPM9fvzz0g8rzNdWRzzdfc/6GUUv74xz+G/Oyzz4as/n7j5Dnzm9/8JuRcD3Xq1KkhT5w4MeRcJ7+UUvr379+hMeVa/B2tDb18+fKar910000hn3/++SEvWLAgZHNw05PPqW+88UbIea4fdNBBIed6nt/5zndCzrXbge7t0ksvDfnmm2+ueU3uo9VRufZ+7i2zePHikHMd4lIa3z+hXp30lStXhrxq1aqQc8+dTN305powYULIxx13XMj1+qHk3lq5h9jYsWNDzvc4ufZ6vr/I14Js2l599dWQf/nLX9a85sUXXwz5zDPPDPmoo44KOdd1b+/+Ij9rmTZtWs0Y8tfuvffekK1lXdvkyZNDHjduXMj5OUYppdxwww0h61m06cs9HXJvyXwddPXVV4d8yy23hPxuzmftjSFfd+kBsXF8EgIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBK9PieEO3JNQVz7c25c+eGrIZ515dr/d5///0hX3XVVSFPmjQp5FxfdcCAASFvt912IeceFKXU1pfL9YjzvPrRj34U8gsvvBDyvHnzQn7llVfa/H4ptXU/1bBrnlzTMPfn+PnPfx5y7q3wgQ98IORcW7PeMXbZZZeQc++TXHv/oYceCvn2228POdfIznO8lNp+Ks8880ybY2TTk2tnPvHEEyHnc+7w4cPb/Pl6tdkhy/MqXwfka7fXX3895Fxbn8ZZuHBhyPn6ppT3Xrc+n3s2lVrT+c+hTnpj5f//cw+5++67L+R8vsv9v7bffvuaY8yaNSvk3BMiXw8ecMABIb/55pttjjFf6z311FM1Y2DTkdeI3KutlNqegs8991zIF1988XsaQ+4VVu8eNp8H8n00XVvuUzh79uyQ8xwrpZTp06dXOia6no5ew+S1I2e6Dp+EAAAAAAAAKmETAgAAAAAAqIRNCAAAAAAAoBJ6QrQj1yJbtGhRyOqrdj+5tnOuSX7RRReFvOOOO4bcu3fvkHPPhyFDhrT5+lJKWb58eciLFy8O+eWXXw4519vM8zD/mdTW795yffI5c+a0mXPN1jwHS6mdE7m28JgxY0LOvU6ef/75kJ9++uk2x1xPrsNunvY8+ZyZe9Pk9Xjo0KEh//73vw9ZfWo2Ru5Rk+uon3feeSE/8sgjIdfrQ0Bj5OvwTaVfA5u+PFfztf5PfvKTkPP58Pjjjw/5sMMOqznGPvvsE3KvXvHfF+ba6zNnzgz52muvDTn36poxY0bI+X6ETVu99TbXWc/9MnOG7MYbbwz5gQceCDn3iCil9h4S6L58EgIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBK6AlBj5f7KeT6pzlDV/PCCy90+Gcee+yxzh8IdFDuJfKDH/wg5NybJNeRzf1QoJ7ci2ThwoUhf//732/kcIAeKK9Dd955Z8h5Xcr9F3beeef3fMx87XfXXXeF/NJLL4W8Mf2+ADrivvvua/YQgCbySQgAAAAAAKASNiEAAAAAAIBK2IQAAAAAAAAqYRMCAAAAAACoRI9vTJ2bEi9evDjklpaWkIcPHx7yZpttVsm4AGBTt3r16pB/8YtfNGkkANA4uWn0zJkz28wAAN2dT0IAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQiR7fEyL3gHjggQdCnjNnTsg777xzyEOHDg15wYIFIed6nwAAAAAA0FP4JAQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACV0BMi9YS48cYbQ16+fHnI5557bsiTJ08OOfeEeO21197jCAEAAAAAoHvySQgAAAAAAKASNiEAAAAAAIBKbFQ5ptbW1qrH0WXkP+vatWtDzuWZ1qxZE/K6deuqGVgX04g50ZPmHRun6jlhzlGPeUejOcfSDNY6Gs1aRzNY62gG845Gc46lGdqbExu1CbFs2bJOGUx3kDcZpk2b1mbuqZYtW1a23HLLyo8Bf6nqeWfOUY95R6M5x9IM1joazVpHM1jraAbzjkZzjqUZ2pt3La0bsXW1bt26Mm/evDJ48ODS0tLSqQOke2ltbS3Lli0rI0aMKL16VVvNy7xjvUbNO3OOv2Te0WjOsTSDtY5Gs9bRDNY6msG8o9GcY2mGjZ13G7UJAQAAAAAA0FEaUwMAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAldh8Y160bt26Mm/evDJ48ODS0tJS9ZjowlpbW8uyZcvKiBEjSq9e1e5hmXes16h5Z87xl8w7Gs05lmaw1tFo1jqawVpHM5h3NJpzLM2wsfNuozYh5s2bV0aPHt1pg6P7mzt3bhk1alSlxzDvyKqed+Yc9Zh3NJpzLM1graPRrHU0g7WOZjDvaDTnWJqhvXm3UZsQgwcPLqWU8u///u+lf//+pZRSvvCFL4TXvPXWW+9yiDTasGHDar42ZcqUkDfbbLOQr7rqqlJKKUuXLi2jR4/eMCeqtP4YZ511VunTp08ppZSLL744vKa1tbXycdA5hg4dWvO1yZMnh7z55nFJuvrqq0spjZt3699/7ty5ZYsttiillLL11luH16xevbrSMdB5evfuXfO1PA/zv9g46aSTNvzv1atXlx//+McNm3cPPvhgGTRoUCmllL333ju8Zvny5ZWOgc5Tb62bNGlSyPlfh1x77bWllOacY+++++4N8+6AAw4IrzHvuo/uMO/qnWNHjhwZXvPmm29WOgY6T71/8divX782X7P+uu/tt98uDz30UEPXur+cd/nmeNmyZZWPg+rktS3Pu+HDh5dS3vkXu6+99pq1jg7J96ellA2/1/XynPvwhz+84X+vXr26XHHFFQ2bd+eee27p27dvKaWU//zP/wyvcR/bfdSbd+3NoYMPPriUUsqaNWvKDTfc0NBz7HXXXVcGDhxYSinlmGOOCa9ZsWJF5eOgcwwZMqTma7vsskvI+VnxZZddVkp551pqypQp7c67jdqEWL+o9u/ff8MmhI/adF/1PhqTF7k8sdo70VZh/TH69Omz4USaj2sTovvYmHmXc6Pn3fr332KLLTYc21rXfdX73bV3o7p+rWnvfTrT+vcfNGjQhpO2edd91fvd5bUtz8NmnmPNu01Dd5h3zrGblnq/u/y1nPOcbORa917mXb3XuwfpOtqbd+1d+1U1HmvdpuHd3E+s/weU7b1PZ1r//n379v0/n53Qfbybc2z+B3iNPMcOHDhwwz9qMu+6r425n3ivz4pbWjfiCmrp0qVlyy23bO9l9CBLliypmWydzbxj/QLX2tpa1q1bV/m8M+eox7yjautvZltbW0tra6tzLA3hHEuzWetoBmsdVfvLh3DrH7eZdzSacyzN0N68q7ZLCQAAAAAA0GPZhAAAAAAAACphEwIAAAAAAKjERjWmBmiGt99+u9lDAKjcunXrmj0EeiDnWADofBrXA9TnkxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAA0EW0tLSUlpaWZg8DoFLWOprBvKPRzDkA+DObEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFRi82YPAAB4R2tra7OHAFA5ax3NYN7RaOYcAPyZT0IAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAEAX0dLSUlpaWpo9DADY5DjH0mjmHAD8mU0IAAAAAACgEjYhAAAAAACAStiEAAAAAAAAKrF5swcAALyjtbW12UMAgE2ScyyNZs4BwJ/5JAQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEAAAAAABQCZsQAAAAAABAJWxCAAAAAAAAlbAJAQAAAAAAVMImBAAAAAAAUAmbEAAAAAAAQCVsQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFAJmxAAAAAAAEAlbEIAAAAAAACVsAkBAAAAAABUwiYEQB0tLS2lpaWl2cOghzHvAKAazrE0mjkHAH9mEwIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBKbN7sAQB0Ra2trc0eAj2QeQcA1XCOpdHMOQD4M5+EAAAAAAAAKmETAgAAAAAAqIRNCAAAAAAAoBI2IQAAAAAAgErYhAAAAAAAACphEwIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBK2IQAAAAAAAAqYRMCAAAAAACohE0IAAAAAACgEpt39AdaWlpKKaW0trZ2+mAAugprHc1g3tEM5l3P5PdOT2PO02jmHNAovXq982/MW1tbm7LmWO96po7OO5+EAAAAAAAAKmETAgAAAAAAqIRNCAAAAAAAoBI2IQAAAAAAgEp0qDF1S0uLZiM91HbbbVdKKWXdunXllVdeafjxzbueabPNNiulvPN7X7duXcOO26tXrw1z7u23327YcenZzLueq5nnWPOu5zrssMNKKaWsXbu23HHHHc0dDD1CM6/nrXU915AhQ0op78y7JUuWNOy45hzN4tlJz3TccceVUkpZs2ZNuf766xt6bM+Ke66jjjqqlPLO/cS0adPafb1PQgAAAAAAAJWwCQEAAAAAAFTCJgQAAAAAAFCJDvWEGD16dOnV6519ixdeeKGK8dBFjRs3rpTyTp2vRterHj58+IZ59+qrrzb02DRXnz59Sinv1BVctWpVw467xx57bOhH8eCDDzbsuPRskyZN2jDvHn300SaPhkZq5jl211133TDvHnvssYYem+Y64YQTSimlrFq1Sk8IGmL99Xyje32V4tquJ9tmm21KKe/0ZWhkTwhzrudav9aV8s5618ga+dtuu+2G48+fP79hx6Xx1vdgWO9v/uZvSimlrFy5suE9IcaPH79h3j377LMNPTaNlefdSSedVEp5Z97pCQEAAAAAADSNTQgAAAAAAKASNiEAAAAAAIBKdKgnxPHHH1/69u1bSinlu9/9biUDomsYOHBgyLvttlsppZTVq1c3vKblfvvtV3r37l1KKeV3v/tdQ49NY+X6cv379y+lNL4nxFe+8pUyYMCAUkopxxxzTMOOS8927rnnbph3p512WpNHQ5XW/57XmzJlSimlOefYz3/+8xvW2k996lMNPTbV+sua1KWUMmbMmJA/+clPllJKWbp0afnyl7/csHGV8ufzfSPrZNN866/nW1tby1tvvdXQY3/lK1/ZcH/zoQ99qKHHprHW95Rbb+zYsaWUd/ouPffccw0bx1e/+tUNc27q1KkNOy7Nt74XSCnvrHdr165t2LEPPvjgDWvt5Zdf3rDj0rnyc5FS4rwqpZQJEyaE/IlPfKKU8s513dlnn13d4Oo4+eSTS79+/UoppVxwwQUNPTadp968y/cT63sZrnfWWWeVUt6Zd+ecc067x/BJCAAAAAAAoBI2IQAAAAAAgEpsVDmm9R+VXr16daWDoevIH49f/7tf/99GfHx+/THWrFlT+bHoGvK8Wp/zf6s+/ooVKyo9Dt1Lo+bdypUrKz0OXUdXOsead5uuPI/WrVsX8tKlS0sppSxbtqzu66scjzJMPVOjr+v+8hiu7XqOPK/Wl8JZ/1/3E1St3vmuUfPOs5NNQ735kr/29ttvh7z+um79fxt5jm10iUWqsTHz7v+6n9jYedfSuhEz86WXXiqjR49u72X0IHPnzi2jRo2q9BjmHVnV886cox7zjkZzjqUZrHU0mrWOZrDW0QzmHY3mHEsztDfvNmoTYt26dWXevHll8ODBdRtV0HO0traWZcuWlREjRtQ0KOls5h3rNWremXP8JfOORnOOpRmsdTSatY5msNbRDOYdjeYcSzNs7LzbqE0IAAAAAACAjtKYGgAAAAAAqIRNCAAAAAAAoBI2IQAAAAAAgErYhAAAAAAAACphEwIAAAAAAKiETQgAAAAAAKASNiEAAAAAAIBK/H+6JZZucE8TZAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(img[i].cpu().numpy().reshape(32, 32))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(output[i].cpu().numpy().reshape(32, 32))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the deeper convolutional autoencoder architecture with dense layers\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # Assuming grayscale images\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*3*3, 512),  # Added dense layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256)  # Added dense layer\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),  # Added dense layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),  # Added dense layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256*3*3),  # Added dense layer\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 3, 3)),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output pixel values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.shape)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Load your dataset and preprocess if necessary\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor\n",
    "    transforms.Resize((32,32)),\n",
    "])\n",
    "\n",
    "# Example of loading MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Instantiate the deeper model\n",
    "model = DeepAutoencoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x1024 and 2304x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, images)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 49\u001b[0m, in \u001b[0;36mDeepAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ivp/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x1024 and 2304x512)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)  # Compute the loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
